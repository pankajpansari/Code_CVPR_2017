\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\citation{ladickyeccv10}
\citation{ladickyeccv10}
\@writefile{toc}{\contentsline {section}{\numberline {1}Experiments}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Synthetic Data}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Data.}{1}{section*.1}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{1}{1.1}{section*.1}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{1}{1.1}{section*.1}}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Method.}{1}{section*.2}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{1}{1.1}{section*.2}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{1}{1.1}{section*.2}}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Results.}{1}{section*.3}}
\@writefile{brf}{\backcite{ladickyeccv10}{{1}{1.1}{section*.3}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{1}{1.1}{section*.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot is the same as shown in the paper, but we include it here for the sake of comparison. Red dot indicates convergence of parsimonious labeling algorithm and dotted line indicates extrapolation.}}{2}{figure.1}}
\newlabel{fig:linear_weight5}{{1}{2}{\footnotesize \em Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot is the same as shown in the paper, but we include it here for the sake of comparison. Red dot indicates convergence of parsimonious labeling algorithm and dotted line indicates extrapolation}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot corresponds to the same experiment as mentioned in the paper, but with results for co-occurrence included. Red and black dots indicate convergence of respective algorithms and dotted line indicates extrapolation.}}{2}{figure.2}}
\newlabel{fig:linear_weight5_cooc}{{2}{2}{\footnotesize \em Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot corresponds to the same experiment as mentioned in the paper, but with results for co-occurrence included. Red and black dots indicate convergence of respective algorithms and dotted line indicates extrapolation}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 10$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. Parsimonious labeling performs well for $M$ = 5, but our approach outperforms for higher values of $M$. Red dot indicates convergence of parsimonious labeling and dotted line indicates extrapolation.}}{3}{figure.3}}
\newlabel{fig:linear_weight10}{{3}{3}{\footnotesize \em Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 10$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. Parsimonious labeling performs well for $M$ = 5, but our approach outperforms for higher values of $M$. Red dot indicates convergence of parsimonious labeling and dotted line indicates extrapolation}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot corresponds to the same experiment as in figure \nobreakspace  {}\ref  {fig:linear_weight10}, but with results for co-occurrence included. Red and black dots indicate convergence of respective algorithms and dotted line indicates extrapolation.}}{3}{figure.4}}
\newlabel{fig:linear_weight10_cooc}{{4}{3}{\footnotesize \em Results for synthetic data using truncated linear distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use truncation factors as $M$ = 5, 10 and 15 and $m$ = 1, and for each we vary interval lengths for our algorithm. This plot corresponds to the same experiment as in figure ~\ref {fig:linear_weight10}, but with results for co-occurrence included. Red and black dots indicate convergence of respective algorithms and dotted line indicates extrapolation}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Results for synthetic data using truncated quadratic distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use $M$ = 25, 100 and 225, and for each we vary interval lengths for our algorithm.}}{4}{figure.5}}
\newlabel{fig:quadratic_weight5}{{5}{4}{\footnotesize \em Results for synthetic data using truncated quadratic distance function. The plots show the variation of energy versus time, averaged over 50 lattices using $\omega _c = 5$. We use $M$ = 25, 100 and 225, and for each we vary interval lengths for our algorithm}{figure.5}{}}
\citation{comaniciu2002mean}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Real Data}{5}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Image Inpainting and Denoising}{5}{subsubsection.1.2.1}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Data.}{5}{section*.4}}
\@writefile{brf}{\backcite{comaniciu2002mean}{{5}{1.2.1}{section*.4}}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Method.}{5}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Results.}{5}{section*.6}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{5}{1.2.1}{section*.6}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{5}{1.2.1}{section*.6}}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{5}{1.2.1}{section*.6}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{5}{1.2.1}{section*.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Image inpainting results for `penguin'. Note that comparison with (b) and (c) makes sense only for $m$ = 1. Also, we restricted our experiments to smaller (and suboptimal) $L$ due to computational issues.}}{6}{figure.6}}
\newlabel{fig:inpainting_results}{{6}{6}{\footnotesize \em Image inpainting results for `penguin'. Note that comparison with (b) and (c) makes sense only for $m$ = 1. Also, we restricted our experiments to smaller (and suboptimal) $L$ due to computational issues}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Image inpainting results for `house'. Note that comparison with (b) and (c) makes sense only for $m$ = 1. Also, we restricted our experiments to smaller (and suboptimal) $L$ due to computational issues.}}{7}{figure.7}}
\newlabel{fig:inpainting_results}{{7}{7}{\footnotesize \em Image inpainting results for `house'. Note that comparison with (b) and (c) makes sense only for $m$ = 1. Also, we restricted our experiments to smaller (and suboptimal) $L$ due to computational issues}{figure.7}{}}
\citation{comaniciu2002mean}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Stereo Matching}{8}{subsubsection.1.2.2}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Data.}{8}{section*.7}}
\@writefile{brf}{\backcite{comaniciu2002mean}{{8}{1.2.2}{section*.7}}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Method.}{8}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Results.}{8}{section*.9}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{8}{1.2.2}{section*.9}}}
\@writefile{brf}{\backcite{ladickyeccv10}{{8}{1.2.2}{section*.9}}}
\citation{ladickyeccv10}
\citation{dokaniaiccv15}
\citation{ladickyeccv10}
\citation{dokaniaiccv15}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Stereo matching results: Figures (a), (f), (k) and (p) are the ground truth disparity for `tsukuba', `teddy', `venus' and `cones' respectively. Our results for $m$ = 1 (d), (i), (n) and (s) are significantly better than those of\nobreakspace  {}\cite  {ladickyeccv10} (b), (g), (l) and (q), and of\nobreakspace  {}\cite  {dokaniaiccv15} (c), (h), (m) and (r) in terms of energy. We also show results for $m$ = 3. We use super-pixels obtained using mean-shift as cliques. $^*$Note that $m$ = 3 uses a different energy function from other cases.}}{9}{figure.8}}
\@writefile{brf}{\backcite{ladickyeccv10}{{9}{8}{figure.8}}}
\@writefile{brf}{\backcite{dokaniaiccv15}{{9}{8}{figure.8}}}
\newlabel{fig:stereo_matching}{{8}{9}{\footnotesize \em Stereo matching results: Figures (a), (f), (k) and (p) are the ground truth disparity for `tsukuba', `teddy', `venus' and `cones' respectively. Our results for $m$ = 1 (d), (i), (n) and (s) are significantly better than those of~\cite {ladickyeccv10} (b), (g), (l) and (q), and of~\cite {dokaniaiccv15} (c), (h), (m) and (r) in terms of energy. We also show results for $m$ = 3. We use super-pixels obtained using mean-shift as cliques. $^*$Note that $m$ = 3 uses a different energy function from other cases}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Advantages of using TMCM}{10}{section.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Clique potential value $\theta _{\bf  c}({\bf  x}_{\bf  c})$ defined by a linear function with $M$ = 3 and $\bf  {\omega _c}$ = 1 for various values of $m$. Since clique size is 6, $0 \leq m \leq 3$. Pair (a) demonstrates why taking the largest convex distances favors smoothness; (b) demonstrates how truncation prevents overpenalization of discontinuities; (c) demonstrates how using $m > 1$ can provide some degree of robustness to errors in the definitions of the cliques. }}{10}{table.1}}
\newlabel{table:cliqueExample}{{1}{10}{\footnotesize \em Clique potential value $\theta _{\bf c}({\bf x}_{\bf c})$ defined by a linear function with $M$ = 3 and $\bf {\omega _c}$ = 1 for various values of $m$. Since clique size is 6, $0 \leq m \leq 3$. Pair (a) demonstrates why taking the largest convex distances favors smoothness; (b) demonstrates how truncation prevents overpenalization of discontinuities; (c) demonstrates how using $m > 1$ can provide some degree of robustness to errors in the definitions of the cliques}{table.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Smoothness.}{10}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Discontinuities.}{10}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Robustness.}{10}{section*.12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Definitions of various symbols used in the supplementary document}}{11}{table.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Optimization via Range Expansion}{12}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Range Expansion Algorithm for TMCM}{12}{subsection.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The range expansion algorithm for TMCM.}}{12}{algorithm.1}}
\newlabel{eq:rangeMove}{{1}{12}{Range Expansion Algorithm for TMCM}{equation.3.1}{}}
\newlabel{algo:rangeExpansion}{{1}{12}{Range Expansion Algorithm for TMCM}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Overestimation of the Energy Function}{13}{subsection.3.2}}
\newlabel{subsec:overestimate}{{3.2}{13}{Overestimation of the Energy Function}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Unary Potentials.}{13}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Clique Potentials.}{13}{section*.14}}
\newlabel{eq:submodOverestimate}{{3}{13}{Clique Potentials}{equation.3.3}{}}
\citation{boykovpami04}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Arcs and their capacities for representing the unary potentials for the random variable $X_a$. According to the labeling defined in equation\nobreakspace  {}(\ref  {eq:labeling}), if $x_a = \mathaccentV {hat}05E{x}_a$, then the arc $(s,V^a_1)$ will be cut, which will contribute exactly $\theta _a(\mathaccentV {hat}05E{x}_a)$ to the capacity of the cut. If $x_a = s+i-1$ where $i \in \{1,\cdots  ,h'-1\}$, then the arc $(V^a_i,V^a_{i+1})$ will be cut, which will contribute exactly $\theta _a(s+i-1)$ to the capacity of the cut. If $x_a = l$, then the arc $(V^a_{h'},t)$ will be cut, which will contribute exactly $\theta _a(l)$ to the capacity of the cut. The arcs with infinite capacity ensure that exactly one of the arcs from the set $(s,V^a_1) \cup \{(V^a_i,V^a_{i+1}), i=1,\cdots  ,h'-1\} \cup (V^a_{h'},t)$ will be part of an $st$-cut with finite capacity, which will guarantee that we are able to obtain a valid labeling. }}{14}{figure.9}}
\newlabel{fig:unary}{{9}{14}{\footnotesize \em Arcs and their capacities for representing the unary potentials for the random variable $X_a$. According to the labeling defined in equation~(\ref {eq:labeling}), if $x_a = \hat {x}_a$, then the arc $(s,V^a_1)$ will be cut, which will contribute exactly $\theta _a(\hat {x}_a)$ to the capacity of the cut. If $x_a = s+i-1$ where $i \in \{1,\cdots ,h'-1\}$, then the arc $(V^a_i,V^a_{i+1})$ will be cut, which will contribute exactly $\theta _a(s+i-1)$ to the capacity of the cut. If $x_a = l$, then the arc $(V^a_{h'},t)$ will be cut, which will contribute exactly $\theta _a(l)$ to the capacity of the cut. The arcs with infinite capacity ensure that exactly one of the arcs from the set $(s,V^a_1) \cup \{(V^a_i,V^a_{i+1}), i=1,\cdots ,h'-1\} \cup (V^a_{h'},t)$ will be part of an $st$-cut with finite capacity, which will guarantee that we are able to obtain a valid labeling}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Graph Construction}{14}{subsection.3.3}}
\newlabel{subsec:graph}{{3.3}{14}{Graph Construction}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Description}{14}{subsubsection.3.3.1}}
\@writefile{brf}{\backcite{boykovpami04}{{14}{3.3.1}{subsubsection.3.3.1}}}
\newlabel{eq:labeling}{{5}{14}{Description}{equation.3.5}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Representing Unary Potentials.}{14}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \em  Arcs used to represent the high-order potentials for the clique ${\bf  X}_{\bf  c} = \{X_1,X_2,\cdots  ,X_c\}$. {\bf  Left.} The term $r_{ij}$ is defined in equation\nobreakspace  {}(\ref  {eq:convexCapacity}). The arcs represent the sum of the $m$ maximum convex distance functions over disjoint pairs of random variables when no random variable retains its old label. These arcs are specified only for $i \leq j$ and when either one or both of $i$ and $j$ are not equal to 1. {\bf  Right.} The terms $A$ and $B$ are defined in equation\nobreakspace  {}(\ref  {eq:submodCapacity}). The arcs represent an overestimation of the clique potential for the case where some or all the random variables retain their old label. }}{15}{figure.10}}
\newlabel{fig:clique}{{10}{15}{\footnotesize \em Arcs used to represent the high-order potentials for the clique ${\bf X}_{\bf c} = \{X_1,X_2,\cdots ,X_c\}$. {\bf Left.} The term $r_{ij}$ is defined in equation~(\ref {eq:convexCapacity}). The arcs represent the sum of the $m$ maximum convex distance functions over disjoint pairs of random variables when no random variable retains its old label. These arcs are specified only for $i \leq j$ and when either one or both of $i$ and $j$ are not equal to 1. {\bf Right.} The terms $A$ and $B$ are defined in equation~(\ref {eq:submodCapacity}). The arcs represent an overestimation of the clique potential for the case where some or all the random variables retain their old label}{figure.10}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Representing Clique Potentials.}{15}{section*.16}}
\newlabel{eq:convexCapacity}{{6}{15}{\bf Representing Clique Potentials}{equation.3.6}{}}
\newlabel{eq:submodCapacity}{{7}{15}{\bf Representing Clique Potentials}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Properties of the Graph}{15}{subsubsection.3.3.2}}
\newlabel{subsec:graph_properties}{{3.3.2}{15}{Properties of the Graph}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Proofs}{16}{subsubsection.3.3.3}}
\newlabel{lemma:cliqueGraphProof}{{1}{16}{}{lemma.1}{}}
\newlabel{eq:partition}{{9}{17}{Proofs}{equation.3.9}{}}
\newlabel{eq:regrouping}{{10}{17}{Proofs}{equation.3.10}{}}
\newlabel{prop:graphCut}{{1}{18}{}{proposition.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Energy Minimization.}{18}{section*.17}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Multiplicative Bounds}{19}{section.4}}
\newlabel{sec:Bounds}{{4}{19}{Multiplicative Bounds}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bounds for Truncated Max-of-linear Models}{19}{subsection.4.1}}
\newlabel{lemma:reductionlowerBound}{{2}{19}{}{lemma.2}{}}
\newlabel{lemma:linearIneq}{{3}{20}{}{equation.4.12}{}}
\newlabel{eq:rewriteIneq}{{14}{20}{Bounds for Truncated Max-of-linear Models}{equation.4.14}{}}
\newlabel{eq:case1bound}{{15}{21}{Bounds for Truncated Max-of-linear Models}{equation.4.15}{}}
\newlabel{eq:Aexpr}{{16}{22}{Bounds for Truncated Max-of-linear Models}{equation.4.16}{}}
\newlabel{eq:Bexpr}{{17}{22}{Bounds for Truncated Max-of-linear Models}{equation.4.17}{}}
\newlabel{eq:Cexpr}{{18}{23}{Bounds for Truncated Max-of-linear Models}{equation.4.18}{}}
\newlabel{eq:sum_ABC}{{19}{23}{Bounds for Truncated Max-of-linear Models}{equation.4.19}{}}
\newlabel{eq:case2bound}{{20}{23}{Bounds for Truncated Max-of-linear Models}{equation.4.20}{}}
\newlabel{eq:rewrite_sum_ABC}{{4.1}{24}{Bounds for Truncated Max-of-linear Models}{equation.4.20}{}}
\newlabel{eq:case3bound}{{21}{24}{Bounds for Truncated Max-of-linear Models}{equation.4.21}{}}
\newlabel{prop:linearBound}{{2}{24}{}{proposition.2}{}}
\newlabel{eq:linear_bound}{{22}{24}{}{equation.4.22}{}}
\newlabel{unary_sum}{{22}{25}{Bounds for Truncated Max-of-linear Models}{equation.4.22}{}}
\newlabel{eq:upperboundfinalenergy}{{23}{25}{Bounds for Truncated Max-of-linear Models}{equation.4.23}{}}
\newlabel{eq:linearboundEquality}{{25}{25}{Bounds for Truncated Max-of-linear Models}{equation.4.25}{}}
\newlabel{eq:Loptimum}{{26}{25}{Bounds for Truncated Max-of-linear Models}{equation.4.26}{}}
\newlabel{eq:Loptimumbound}{{27}{25}{Bounds for Truncated Max-of-linear Models}{equation.4.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Bounds for $m$-Truncated Max-of-linear Models}{26}{subsection.4.2}}
\newlabel{lemma:mreductionlowerBound}{{4}{26}{}{lemma.4}{}}
\newlabel{lemma:mlinearIneq}{{5}{26}{}{equation.4.28}{}}
\newlabel{eq:mrewriteIneq}{{29}{27}{Bounds for $m$-Truncated Max-of-linear Models}{equation.4.29}{}}
\newlabel{eq:mcase1bound}{{30}{28}{Bounds for $m$-Truncated Max-of-linear Models}{equation.4.30}{}}
\newlabel{eq:mcase2bound}{{31}{28}{Bounds for $m$-Truncated Max-of-linear Models}{equation.4.31}{}}
\newlabel{eq:mcase3bound}{{32}{28}{Bounds for $m$-Truncated Max-of-linear Models}{equation.4.32}{}}
\newlabel{prop:mlinearBound}{{3}{28}{}{proposition.3}{}}
\newlabel{eq:mlinear_bound}{{33}{28}{}{equation.4.33}{}}
\bibstyle{plain}
\bibdata{suppplementary}
